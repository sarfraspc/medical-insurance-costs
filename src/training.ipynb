{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28d0a21",
   "metadata": {},
   "source": [
    "After feature engineering, I saved the cleaned dataset to a CSV for easy reuse.\n",
    "now loading it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe13e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('../data/cleaned.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4a031",
   "metadata": {},
   "source": [
    "Removed the auto-generated index column to keep things clean and model-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907e7b0",
   "metadata": {},
   "source": [
    "then separated features (X) and target (y) and split them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6a3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=df.drop('charges',axis=1)\n",
    "y=df['charges']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b23c134",
   "metadata": {},
   "source": [
    "80-20 Split to ensure the model learns patterns and still gets tested on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566494d8",
   "metadata": {},
   "source": [
    "Before training, I standardized the numerical features to bring them on the same scale using StandardScaler. Because models like Linear Regression and SVM are sensitive to feature scales.\n",
    "especially important for engineered features like smoker_bmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94dec43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar=StandardScaler()\n",
    "cols=['age','bmi','children','smoker_bmi','smoker_children']\n",
    "\n",
    "X_train_scaled=scalar.fit_transform(X_train[cols])\n",
    "X_test_scaled=scalar.transform(X_test[cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b729ce8",
   "metadata": {},
   "source": [
    "After scaling selected numerical features, I combined them with the categorical ones (like sex, smoker, and region dummies) using np.hstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b2e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_lin = np.hstack([X_train_scaled, X_train.drop(cols, axis=1).values])\n",
    "X_test_lin = np.hstack([X_test_scaled, X_test.drop(cols, axis=1).values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020d3de",
   "metadata": {},
   "source": [
    "Not all features need scaling.\n",
    "\n",
    "numerical features like age, bmi, children, smoker_bmi, and smoker_children vary a lot in range — and models like Linear Regression, Ridge, SVR are sensitive to that.\n",
    "\n",
    "binary/categorical features (e.g., sex, smoker, and one-hot regions) are already in 0/1 format — scaling them might break their meaning.\n",
    "\n",
    "So I only scaled the numerical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537aa6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_train shape: (1069, 10)\n",
      "X_train_scaled shape : (1069, 5)\n",
      "X_train_lin shape: (1069, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original X_train shape:\", X_train.shape)\n",
    "print(\"X_train_scaled shape :\", X_train_scaled.shape)\n",
    "print(\"X_train_lin shape:\", X_train_lin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6d5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "\n",
    "linear=LinearRegression()\n",
    "linear.fit(X_train_lin,y_train)\n",
    "linear_pred=linear.predict(X_test_lin)\n",
    "linear_pred_real=np.expm1(linear_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2ace09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_ridge={\n",
    "    'alpha':[0.1,1.0,5,10]\n",
    "}\n",
    "ridge=GridSearchCV(Ridge(),param_grid=param_grid_ridge,cv=3)\n",
    "ridge.fit(X_train_lin,y_train)\n",
    "ridge_pred=ridge.predict(X_test_lin)\n",
    "ridge_pred_real=np.expm1(ridge_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "poly_model=make_pipeline(PolynomialFeatures(),LinearRegression())\n",
    "param_grid_poly={\n",
    "    'polynomialfeatures__degree':[2,3,4,5]\n",
    "}\n",
    "\n",
    "poly=GridSearchCV(poly_model,param_grid=param_grid_poly,cv=3)\n",
    "poly.fit(X_train_lin,y_train)\n",
    "poly_pred=poly.predict(X_test_lin)\n",
    "poly_pred_real=np.expm1(poly_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6589c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid_svr={\n",
    "    'epsilon':[0.1,0.5],\n",
    "    'C':[0.1,1],\n",
    "    'kernel':['linear']\n",
    "}\n",
    "\n",
    "scalr_y=StandardScaler()\n",
    "y_train_scaled=scalr_y.fit_transform(y_train.values.reshape(-1,1)).ravel()\n",
    "\n",
    "svr=GridSearchCV(SVR(),param_grid=param_grid_svr,cv=2,n_jobs=-1)\n",
    "svr.fit(X_train_lin,y_train_scaled)\n",
    "svr_pred_scaled=svr.predict(X_test_lin)\n",
    "svr_pred=scalr_y.inverse_transform(svr_pred_scaled.reshape(-1,1)).ravel()\n",
    "svr_pred_real=np.expm1(svr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbae89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid_random={\n",
    "    'n_estimators':[100,200],\n",
    "    'min_samples_split':[2,5],\n",
    "    'max_depth':[None,5,10,20]\n",
    "}\n",
    "\n",
    "random=GridSearchCV(RandomForestRegressor(random_state=44),param_grid=param_grid_random,cv=3)\n",
    "random.fit(X_train,y_train)\n",
    "random_pred=random.predict(X_test)\n",
    "random_pred_real=np.expm1(random_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21325c",
   "metadata": {},
   "source": [
    "###  Model Training & Hyperparameter Tuning\n",
    "\n",
    "| Model                  | Key Details                                                 | Hyperparameters Searched                                  | Post-processing                        |\n",
    "|------------------------|-------------------------------------------------------------|-----------------------------------------------------------|----------------------------------------|\n",
    "| **Linear Regression**   | Baseline straight-line model                                | —                                                         | Trained on `log(charges)`, used `exp`  |\n",
    "| **Ridge Regression**    | L2-regularized model to reduce overfitting/multicollinearity| `alpha ∈ {0.1, 1.0, 5, 10}` (GridSearchCV, cv=3)          | Trained on `log(charges)`, used `exp`  |\n",
    "| **Polynomial Regression**| Captures non-linear patterns using polynomial features     | `degree ∈ {2, 3, 4, 5}` (GridSearchCV, cv=3)              | Trained on `log(charges)`, used `exp`  |\n",
    "| **Support Vector Regressor (SVR)** | Margin-based model with scaled target               | `C ∈ {0.1, 1}`, `epsilon ∈ {0.1, 0.5}` (GridSearchCV, cv=2) | Target scaled before training; inverse transform + `exp` applied |\n",
    "| **Random Forest Regressor**  | Ensemble of decision trees (handles non-linearities well) | `n_estimators ∈ {100, 200}`, `max_depth ∈ {None, 5, 10, 20}`, `min_samples_split ∈ {2, 5}` (GridSearchCV, cv=3) | Trained on `log(charges)`, used `exp`  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf370aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "mae 4438.7955217337385\n",
      "mse 73330163.55100016\n",
      "r2 0.46626006357454974 \n",
      "\n",
      "Ridge Regression\n",
      "mae 4429.93234976219\n",
      "mse 73151479.18714501\n",
      "r2 0.46756063316809704 \n",
      "\n",
      "Polynomial Regression\n",
      "mae 2615.2149541153804\n",
      "mse 23229889.20142734\n",
      "r2 0.830919242708126 \n",
      "\n",
      "svr\n",
      "mae 4818.559740056024\n",
      "mse 102255651.15888683\n",
      "r2 0.25572340077042044 \n",
      "\n",
      "Randome Forest\n",
      "mae 2035.8426458599383\n",
      "mse 18264620.832100995\n",
      "r2 0.8670593778918747 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "y_test_real=np.expm1(y_test)\n",
    "def eval(name,ytest,ypred):\n",
    "    print(name)\n",
    "    print('mae',mean_absolute_error(ytest,ypred))\n",
    "    print('mse',mean_squared_error(ytest,ypred))\n",
    "    print('r2',r2_score(ytest,ypred),'\\n')\n",
    "\n",
    "eval('Linear Regression',y_test_real,linear_pred_real)\n",
    "eval('Ridge Regression',y_test_real,ridge_pred_real)\n",
    "eval('Polynomial Regression',y_test_real,poly_pred_real)\n",
    "eval('svr',y_test_real,svr_pred_real)\n",
    "eval('Randome Forest',y_test_real,random_pred_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e3819",
   "metadata": {},
   "source": [
    "###  Model Evaluation Results\n",
    "\n",
    "All models were evaluated using:\n",
    "\n",
    "- **MAE** (Mean Absolute Error) — lower is better  \n",
    "- **MSE** (Mean Squared Error) — lower is better  \n",
    "- **R² Score** — closer to 1 means better fit\n",
    "\n",
    "| Model                | MAE    | MSE         | R² Score |\n",
    "|----------------------|--------|-------------|----------|\n",
    "| Linear Regression    | 4439   | 73,330,163  | 0.466    |\n",
    "| Ridge Regression     | 4430   | 73,151,479  | 0.468    |\n",
    "| Polynomial Regression| 2615   | 23,229,889  | 0.831    |\n",
    "| SVR                  | 4819   | 102,255,651 | 0.256    |\n",
    "| **Random Forest**  | **2036** | **18,264,621** | **0.867**  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0d8317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/random_forest.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(random,'../data/random_forest.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e065c",
   "metadata": {},
   "source": [
    "After evaluating all models, **Random Forest** was the top performer  \n",
    "with the **lowest MAE (2036)** and **highest R² score (0.867)**.  \n",
    "\n",
    "So, I **saved it using `joblib`** for deployment in the Streamlit app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
